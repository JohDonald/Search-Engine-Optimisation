{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "The goal of this challenge was to code an optimised search engine for recipes achieving bespoke results (/i.e. supporting the user's food preferences) with minimal wall time. A data set is provided. The search engine is to be pretty basic, returning all recipes that contain all of the provided keywords. However, the user can choose from a number of orderings depending on their food preferences, which the search engine should support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification\n",
    "\n",
    "The system provides a function ``search``, with the following specification:\n",
    "```\n",
    "def search(query, ordering = 'normal', count = 10):\n",
    "  ...\n",
    "```\n",
    "\n",
    "It `print`s out the results of the search, subject to the following rules:\n",
    "1. It selects from the set of all recipes that contain __all__ of the words in the query (the positions/order of the words in the recipe are to be ignored).\n",
    "2. It orders them based on the provided ordering (a string, meaning defined below).\n",
    "3. It `print`s the top `count` matches only, preserving the order from best to worst. The search engine `print`s just their title, one per line.\n",
    "\n",
    "\n",
    "### Data set\n",
    "\n",
    "A file, `recipes.json` is provided, containing 17K recipes. It can be parsed into a Python data structure using the [`json`](https://docs.python.org/3/library/json.html) module. It is a list, where each recipe is a dictionary containing various keys:\n",
    "* `title` : Name of recipe; you can assume these are unique\n",
    "* `categories` : A list of tags assigned to the recipe\n",
    "* `ingredients` : What is in it, as a list\n",
    "* `directions` : List of steps to make the recipe\n",
    "* `rating` : A rating, out of 5, of how good it is\n",
    "* `calories` : How many calories it has\n",
    "* `protein` : How much protein is in it\n",
    "* `fat` : How much fat is in it\n",
    "\n",
    "Note that the data set was obtained via web scrapping and hence is noisy - every key except for `title` is missing from at least one recipe. The code will need to cope with this.\n",
    "\n",
    "The data set comes from https://www.kaggle.com/hugodarwood/epirecipes/version/2 though it has undergone some preprocessing/cleaning, i.e. removing of duplicates and really 'dodgy' entries.\n",
    "\n",
    "\n",
    "\n",
    "### Search\n",
    "\n",
    "The search should check the following parts of the recipe (see data set description below):\n",
    "* `title`\n",
    "* `categories`\n",
    "* `ingredients`\n",
    "* `directions`\n",
    "\n",
    "For instance, given the query \"banana cheese\" you would expect \"Banana Layer Cake with Cream Cheese Frosting\" in the results. Note that case is ignored (\"banana\" matches \"Banana\") and the words __do not__ have to be next to one another, in the same order as the search query or even in the same part of the recipe (\"cheese\" could appear in the title and \"banana\" in the ingredients). However, all words in the search query __must__ appear somewhere.\n",
    "\n",
    "\n",
    "\n",
    "### Tokenisation\n",
    "\n",
    "In order to match words, tokenisation is carried out as follows:\n",
    "1. All punctuation and digits are converted into spaces. For punctuation I use the set in [`string.punctuation`](https://docs.python.org/3/library/string.html#string.punctuation), for digits [`string.digits`](https://docs.python.org/3/library/string.html#string.digits).\n",
    "2. [`split()`](https://docs.python.org/3/library/stdtypes.html#str.split) to extract individual tokens.\n",
    "3. Ignore any token that is less than $3$ characters long.\n",
    "4. Make tokens lowercase.\n",
    "\n",
    "When matching words for search it's only a match if the entire token is matched (There are many scenarios where this simple approach will fail, but it's good enough for this exercise). When carrying out a search the code should ignore terms in the search string that fail the above requirements.\n",
    "\n",
    "\n",
    "\n",
    "### Ordering\n",
    "\n",
    "There are three ordering modes to select from / that the search engine will offer, each indicated by passing a string to the `search` function:\n",
    "* `normal` - Based simply on the number of times the search terms appear in the recipe. A score is calculated and the order is highest to lowest. The score sums the following terms (repeated words are counted multiple times, i.e. \"cheese cheese cheese\" is $3$ matches to \"cheese\"):\n",
    "    * $8 \\times$ Number of times a query word appears in the title\n",
    "    * $4 \\times$ Number of times a query word appears in the categories\n",
    "    * $2 \\times$ Number of times a query word appears in the ingredients\n",
    "    * $1 \\times$ Number of times a query word appears in the directions\n",
    "    * The `rating` of the recipe (if not available assume $0$).\n",
    "\n",
    "* `simple` - Tries to minimise the complexity of the recipe, for someone who is in a rush. Orders to minimise the number of ingredients multiplied by the numbers of steps in the directions.\n",
    "\n",
    "* `healthy` - Order from lowest to highest by this cost function:\n",
    "$$\\frac{|\\texttt{calories} - 510n|}{510} + 2\\frac{|\\texttt{protein} - 18n|}{18} + 4\\frac{|\\texttt{fat} - 150n|}{150}$$\n",
    "Where $n \\in \\mathbb{N}^+$ is selected to minimise the cost ($n$ is a positive integer and $n=0$ is not allowed). This can be understood in terms of the numbers $510$, $18$ and $150$ being a third of the recommended daily intake (three meals per day) for an average person, and $n$ being the number of whole meals the person gets out of cooking/making the recipe. So this tries to select recipes that neatly divide into a set of meals that are the right amount to consume for a healthy, balanced diet.\n",
    "\n",
    "To clarify the use of the ordering string, to get something healthy that contains cheese one might call `search('cheese', 'healthy')`. \n",
    "\n",
    "In the case of a recipe that is missing a key in its dictionary the rules are different for each search mode:\n",
    "* `normal` - Consider a missing entry to be zero matches, but the item is still eligible for inclusion in the results, assuming it can match the search with a different entry.\n",
    "* `simple` - If a recipe is missing either `ingredients` or `directions` it is dropped from such a search result. Because the data is messy if either of these lists is of length $1$ it should be assumed that the list extraction has failed and the recipe is to also be dropped from the search results.\n",
    "* `healthy` - If any of `calories`, `protein` or `fat` is missing the recipe should be dropped from the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Finally, goal is to achieve a search that it's faster than $0.1$ seconds on average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and read json file\n",
    "#import time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open('recipes.json') as recipes:\n",
    "    json_file = json.load(recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create translate table\n",
    "import string\n",
    "\n",
    "#Function tokenize_process:\n",
    "#1) gets rid of numbers and punctuation marks and replace with empty string\n",
    "#2) splits strings into their individual words/tokens\n",
    "#3) remove all strings less than 3 characters\n",
    "#4) make everything lowercase\n",
    "\n",
    "def tokenize_process(strings):\n",
    "    #Here we create the translation table \n",
    "    translation = str.maketrans(string.punctuation + string.digits, ' '*len(string.punctuation+string.digits))\n",
    "    \n",
    "    strings_ = str(strings) \n",
    "    translated = strings_.translate(translation) #1) Translates each punctuation sign or digit into an empty string\n",
    "    trans_split = translated.split() #2) Splits up the string into several strings of individual tokens\n",
    "    \n",
    "    output = []\n",
    "    #3) Here we remove all strings less than 3 characters:\n",
    "    for i in trans_split:\n",
    "        if len(i) >= 3:\n",
    "            output.append(i.lower()) #4) here we append i as a lower case\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list too loop through title categories ingredients and directions\n",
    "look_thru_keys = list(json_file[0].keys())\n",
    "look_thru_keys = look_thru_keys[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary recipe map which has tokens as keys and a list of values, which are indexes of the recipes\n",
    "#Also for each token it checks whether it is in the title, categories, ingredients or directions and adds the\n",
    "#index number several times in accordances with the order rank\n",
    "recipe_map = {}\n",
    "recipe_simple_score = {k : 0 for k in range(len(json_file))} #Here we create an empty scoredictionary for the \n",
    "                                                             #recipes for which we fill with relevant simple score\n",
    "                                                             #in the loop\n",
    "for keys in look_thru_keys:\n",
    "    for i in range(len(json_file)):\n",
    "        try:\n",
    "            tokens = tokenize_process(json_file[i][keys])\n",
    "            if len(json_file[i]['directions']) > 1 and len(json_file[i]['ingredients']) > 1:\n",
    "                recipe_simple_score[i] = len(json_file[i]['directions']) * len(json_file[i]['ingredients'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for j in tokens:\n",
    "            if j in recipe_map.keys():\n",
    "                if keys == 'title':\n",
    "                    for order in range(8):\n",
    "                        recipe_map[j].append(i)\n",
    "                if keys == 'categories':\n",
    "                    for order in range(4):\n",
    "                        recipe_map[j].append(i)\n",
    "                if keys == 'ingredients':\n",
    "                    for order in range(2):\n",
    "                        recipe_map[j].append(i)\n",
    "                if keys == 'directions':\n",
    "                    recipe_map[j].append(i)\n",
    "                        \n",
    "            else:\n",
    "                if keys == 'title':\n",
    "                    recipe_map[j] = [i] * 8\n",
    "                if keys == 'categories':\n",
    "                    recipe_map[j] = [i] * 4\n",
    "                if keys == 'ingredients':\n",
    "                    recipe_map[j] = [i] * 2\n",
    "                if keys == 'directions':\n",
    "                    recipe_map[j] = [i]\n",
    "                    \n",
    "scores = np.array(list(recipe_simple_score.values())) #Turned scores into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Healthy scoring\n",
    "nutrition = ['calories','protein','fat']\n",
    "health_score = np.empty(len(json_file))\n",
    "\n",
    "for h in range(len(json_file)):\n",
    "    h_s = []\n",
    "    if nutrition[0] in json_file[h] and nutrition[1] in json_file[h] and nutrition[2] in json_file[h]:\n",
    "        for n in range(1,11):  \n",
    "            sc = (np.fabs(json_file[h][nutrition[0]]-510*n)/510) + (2*(np.fabs(json_file[h][nutrition[1]]-18*n)/18)) +(4*(np.fabs(json_file[h][nutrition[2]]-150*n)/150))\n",
    "            h_s.append(sc)\n",
    "            min_sc = min(h_s)\n",
    "        health_score[h] = min_sc\n",
    "        \n",
    "    else: \n",
    "        health_score[h] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we create an empty matrix with dimensions no. of rows = recipe_map length i.e. number of tokens\n",
    "#and no. of columns = the no. of recipes\n",
    "token_matrix = np.zeros((len(recipe_map),len(json_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we loop through the tokens/keys in the recipe_map dictionary and define the column index of the matrix \n",
    "#to signify a recipe number and the count of how many times that token appears in the given recipe \n",
    "#as the element in the matrix\n",
    "j = 0\n",
    "for k, i in recipe_map.items():\n",
    "    idx, cnt = np.unique(i, return_counts=True)\n",
    "    token_matrix[j][idx] = cnt\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings array:\n",
    "ratings_array = np.empty(len(json_file))\n",
    "for l in range(len(json_file)):\n",
    "    rating = json_file[l]['rating'] if 'rating' in json_file[l] else 0\n",
    "    ratings_array[l] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx = {k : i for i, k in enumerate(recipe_map)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, ordering = 'normal', count = 10):\n",
    "    tokens_ = tokenize_process(query) #Tokenize query such that to match tokens already existing in json_file.\n",
    "    try: \n",
    "        rows = [query_idx[k] for k in tokens_] #Find relevant rows in token matrix associated with the query\n",
    "        if ordering == 'normal':\n",
    "            columns = np.where((token_matrix[rows]>0).all(0)) #Slice all columns such that only intersects are handled\n",
    "            intersect_counts = token_matrix[rows].sum(axis = 0)[columns] + ratings_array[columns] \n",
    "            \n",
    "            #Sum the counts of the tokens/ matrix elements of the intersecting recipes to get count order\n",
    "            \n",
    "            cols = columns[0]\n",
    "            scores_r = intersect_counts[intersect_counts.argsort()[::-1]] \n",
    "\n",
    "            recipes_to_ret = cols[intersect_counts.argsort()[::-1]][0:count] #Re-order recipe array to follow normal order\n",
    "            \n",
    "            search_res = []\n",
    "            \n",
    "            for i in recipes_to_ret:\n",
    "                search_res.append(json_file[i]['title'])\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        if ordering == 'simple':\n",
    "            columns_1 = np.where((token_matrix[rows]>0).all(0)) #Columns/recipes relevant to query\n",
    "\n",
    "            relevant_sc_1 = scores[columns_1] #Scores corresponding to relevant recipes\n",
    "            bool_ = np.where(relevant_sc_1>0)\n",
    "\n",
    "            columns_2 = columns_1[0][bool_[0]]\n",
    "            relevant_sc_2 = relevant_sc_1[bool_[0]]\n",
    "\n",
    "            recipes_to_ret_s = columns_2[relevant_sc_2.argsort()][0:count]\n",
    "            \n",
    "            search_res = []\n",
    "            \n",
    "            for i in recipes_to_ret_s:\n",
    "                search_res.append(json_file[i]['title'])\n",
    "\n",
    "        if ordering == 'healthy':\n",
    "            columns_2 = np.where((token_matrix[rows]>0).all(0))\n",
    "            relevant_sc_2 = health_score[columns_2]\n",
    "            bool_2 = np.where(relevant_sc_2!=0)\n",
    "            columns_3 = columns_2[0][bool_2[0]]\n",
    "\n",
    "            relevant_sc_3 = relevant_sc_2[bool_2[0]]\n",
    "            relevant_sc_3\n",
    "\n",
    "            recipes_to_ret_h = columns_3[relevant_sc_3.argsort()][0:count]\n",
    "            \n",
    "            search_res = []\n",
    "            \n",
    "            for i in recipes_to_ret_h:\n",
    "                search_res.append(json_file[i]['title'])\n",
    "                \n",
    "        return search_res\n",
    "                \n",
    "    except KeyError:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 µs ± 2.35 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit search('banana cheese cake', 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banana Layer Cake with Cream Cheese Frosting  \n",
      "\n",
      "Banana Layer Cake with White Chocolate-Cream Cheese Frosting and Walnuts  \n",
      "\n",
      "Fresh Banana Layer Cake  \n",
      "\n",
      "Banana Coconut Crunch Cake  \n",
      "\n",
      "Banana-Pineapple Layer Cake with Cream Cheese Frosting  \n",
      "\n",
      "Mango-Banana Cake  \n",
      "\n",
      "Banana Layer Cake  \n",
      "\n",
      "Persimmon Cake with Cream Cheese Icing  \n",
      "\n",
      "Carrot-Banana Cake  \n",
      "\n",
      "Banana Cake with Sour Cream Frosting  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = search('banana cheese cake', 'normal')\n",
    "for i in results:\n",
    "    print(i, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
